{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1DOvXJZRkjfKfF9oUpCZXSU3hPG3g1h-l","timestamp":1692943666426}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fsb4kbG3z8sI"},"source":["# **SUBSCRIBE to the [channel](https://www.youtube.com/user/19daredevill?sub_confirmation=1) to learn cool things**"]},{"cell_type":"markdown","metadata":{"id":"U0ssL2LZ0QIv"},"source":["![Subscribe](https://media3.giphy.com/media/XGILFirobxDWglaUyj/giphy.gif?cid=ecf05e474afdd7ef5fe2c0fa50f87822d705fbb2613a3b5c&rid=giphy.gif)"]},{"cell_type":"markdown","metadata":{"id":"pocEtO0r4OVI"},"source":["# Project has the following stages:\n","\n","1.   Collecting Images with Expressions\n","1.   Detecting Faces in the Images and saving them\n","2.   Training our classifier on the Faces\n","1.   Recognition of expression in new Images\n"]},{"cell_type":"markdown","metadata":{"id":"WU6utBLU2OnW"},"source":["**Clone Repository**"]},{"cell_type":"code","metadata":{"id":"sZ6y9zfT5bqC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730821308928,"user_tz":-330,"elapsed":10981,"user":{"displayName":"Devi S Naidu","userId":"13934691048356277861"}},"outputId":"2f4ea2c4-f569-427d-ecb0-282e5b5a634e"},"source":["!git clone https://github.com/misbah4064/facial_expressions.git"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'facial_expressions'...\n","remote: Enumerating objects: 14243, done.\u001b[K\n","remote: Total 14243 (delta 0), reused 0 (delta 0), pack-reused 14243 (from 1)\u001b[K\n","Receiving objects: 100% (14243/14243), 240.06 MiB | 35.46 MiB/s, done.\n","Resolving deltas: 100% (232/232), done.\n","Updating files: 100% (14004/14004), done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"lFRxUtt02Sbe"},"source":["**Creating necessary directories**"]},{"cell_type":"code","metadata":{"id":"qTsln3PG677f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730821312694,"user_tz":-330,"elapsed":463,"user":{"displayName":"Devi S Naidu","userId":"13934691048356277861"}},"outputId":"1d0b5db6-90ec-4255-dad2-ee4a562a631e"},"source":["%cd facial_expressions/\n","%mkdir -p data_set/{anger,happy,neutral,sad,surprise}"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/facial_expressions\n"]}]},{"cell_type":"markdown","metadata":{"id":"cy-1cL_q2WcM"},"source":["**Extracting Images with expressions**"]},{"cell_type":"code","metadata":{"id":"FOyaXC3d6_59","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730821544644,"user_tz":-330,"elapsed":444,"user":{"displayName":"Devi S Naidu","userId":"13934691048356277861"}},"outputId":"bc0efbc0-4e6d-452c-d0ee-515b6d0a7bf3"},"source":["import cv2\n","with open('neutral.txt','r') as f:\n","    img = [line.strip() for line in f]\n","for image in img:\n","    loadedImage = cv2.imread(\"images/\"+image)\n","    cv2.imwrite(\"data_set/neutral/\"+image,loadedImage)\n","print(\"done writing\")"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["done writing\n"]}]},{"cell_type":"markdown","metadata":{"id":"3DudXRMR2f3m"},"source":["# **Step 1 : Creating Data Set of Faces**"]},{"cell_type":"code","metadata":{"id":"sdINLeyywaaS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730821578390,"user_tz":-330,"elapsed":10409,"user":{"displayName":"Devi S Naidu","userId":"13934691048356277861"}},"outputId":"4426e4e2-aae7-4f21-928d-c7d294000934"},"source":["import cv2\n","\n","with open('neutral.txt','r') as f:\n","    images = [line.strip() for line in f]\n","\n","face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n","\n","# For each Emotion, enter one numeric face id\n","face_id = input('\\n Enter Emotion id end press <return> ==>  ')\n","\n","count = 0\n","\n","for image in images:\n","    img = cv2.imread(\"data_set/neutral/\"+image)\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    faces = face_detector.detectMultiScale(gray, 1.3, 5)\n","\n","    for (x,y,w,h) in faces:\n","\n","        cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)\n","        count += 1\n","\n","        # Save the captured image into the datasets folder\n","        cv2.imwrite(\"dataset/User.\" + str(face_id) + '.' + str(count) + \".jpg\", gray[y:y+h,x:x+w])\n","\n","print(\"\\n Done creating face data\")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Enter Emotion id end press <return> ==>  4\n","\n"," Done creating face data\n"]}]},{"cell_type":"code","metadata":{"id":"UE1mOVo4xSai","executionInfo":{"status":"ok","timestamp":1730821582292,"user_tz":-330,"elapsed":441,"user":{"displayName":"Devi S Naidu","userId":"13934691048356277861"}}},"source":["%mkdir trainer"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MdOZODKC2nkc"},"source":["# **Step 2: Training Images**"]},{"cell_type":"code","metadata":{"id":"vH1_BZPAxaHc","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1730823813390,"user_tz":-330,"elapsed":525,"user":{"displayName":"Devi S Naidu","userId":"13934691048356277861"}},"outputId":"17c1dad0-1f58-4f00-ea05-96833dd14356"},"source":["import cv2\n","import numpy as np\n","from PIL import Image\n","import os\n","\n","# Path for face image database\n","path = '/content/facial_expressions/data_set'\n","\n","recognizer = cv2.face.LBPHFaceRecognizer_create()\n","detector = cv2.CascadeClassifier(\"/content/facial_expressions/haarcascade_frontalface_default.xml\");\n","\n","# function to get the images and label data\n","def getImagesAndLabels(path):\n","\n","    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]\n","    faceSamples=[]\n","    ids = []\n","\n","    for imagePath in imagePaths:\n","\n","        PIL_img = Image.open(imagePath).convert('L') # convert it to grayscale\n","        img_numpy = np.array(PIL_img,'uint8')\n","\n","        id = int(os.path.split(imagePath)[-1].split(\".\")[1])\n","        faces = detector.detectMultiScale(img_numpy)\n","\n","        for (x,y,w,h) in faces:\n","            faceSamples.append(img_numpy[y:y+h,x:x+w])\n","            ids.append(id)\n","\n","    return faceSamples,ids\n","\n","print (\"\\n [INFO] Training faces....\")\n","faces,ids = getImagesAndLabels(path)\n","recognizer.train(faces, np.array(ids))\n","\n","# Save the model into trainer/trainer.yml\n","recognizer.write('trainer.yml')\n","\n","# Print the numer of Emotions trained and end program\n","print(\"\\n [INFO] {0} Emotions trained. Exiting Program\".format(len(np.unique(ids))))\n"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," [INFO] Training faces....\n"]},{"output_type":"error","ename":"IsADirectoryError","evalue":"[Errno 21] Is a directory: '/content/facial_expressions/data_set/happy'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-aedb1ef5d678>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n [INFO] Training faces....\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetImagesAndLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mrecognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-aedb1ef5d678>\u001b[0m in \u001b[0;36mgetImagesAndLabels\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimagePath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimagePaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mPIL_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convert it to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mimg_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPIL_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3430\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3431\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3432\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/facial_expressions/data_set/happy'"]}]},{"cell_type":"markdown","metadata":{"id":"D_OC8rZD2sI1"},"source":["# **Step 3 : Recognition (Testing)**"]},{"cell_type":"code","metadata":{"id":"-y91jibJxvsn","colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"status":"error","timestamp":1730823017323,"user_tz":-330,"elapsed":614,"user":{"displayName":"Devi S Naidu","userId":"13934691048356277861"}},"outputId":"420b57c7-cfa8-4c7f-91db-ced4fadbd8e6"},"source":["import cv2\n","import numpy as np\n","import os\n","\n","recognizer = cv2.face.LBPHFaceRecognizer_create()\n","recognizer.read('/content/facial_expressions/trainer.yml')\n","cascadePath = \"/content/facial_expressions/haarcascade_frontalface_default.xml\"\n","faceCascade = cv2.CascadeClassifier(cascadePath);\n","\n","font = cv2.FONT_HERSHEY_SIMPLEX\n","\n","#iniciate id counter\n","id = 0\n","\n","# Emotions related to ids: example ==> Anger: id=0,  etc\n","names = ['Anger', 'Happy', 'neutral', 'sad', 'surprise', 'None']\n","\n","# Initialize and start realtime video capture\n","cam = cv2.VideoCapture(0)\n","cam.set(3, 640) # set video widht\n","cam.set(4, 480) # set video height\n","\n","# Define min window size to be recognized as a face\n","minW = 0.1*cam.get(3)\n","minH = 0.1*cam.get(4)\n","\n","# ret, img =cam.read()\n","img = cv2.imread(\"/content/facial_expressions/dd.09.33.jpeg\")\n","# img = cv2.flip(img, -1) # Flip vertically\n","\n","gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n","\n","faces = faceCascade.detectMultiScale(\n","    gray,\n","    scaleFactor = 1.2,\n","    minNeighbors = 5,\n","    minSize = (int(minW), int(minH)),\n","    )\n","\n","for(x,y,w,h) in faces:\n","\n","    cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n","\n","    id, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n","\n","    # Check if confidence is less them 100 ==> \"0\" is perfect match\n","    if (confidence < 100):\n","        id = names[id]\n","        confidence = \"  {0}%\".format(round(100 - confidence))\n","    else:\n","        id = \"unknown\"\n","        confidence = \"  {0}%\".format(round(100 - confidence))\n","\n","    cv2.putText(img, str(id), (x+5,y-5), font, 1, (255,255,255), 2)\n","    cv2.putText(img, str(confidence), (x+5,y+h-5), font, 1, (255,255,0), 1)\n","\n","cv2.imwrite(\"dha.jpg\",img)\n","\n","print(\"\\n [INFO] Done detecting and Image is saved\")\n","cam.release()\n","cv2.destroyAllWindows()"],"execution_count":24,"outputs":[{"output_type":"error","ename":"error","evalue":"OpenCV(4.10.0) /io/opencv_contrib/modules/face/src/facerec.cpp:61: error: (-2:Unspecified error) File can't be opened for reading! in function 'read'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-695e5cbf53cb>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrecognizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBPHFaceRecognizer_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrecognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/facial_expressions/trainer.yml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mcascadePath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/facial_expressions/haarcascade_frontalface_default.xml\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfaceCascade\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcascadePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv_contrib/modules/face/src/facerec.cpp:61: error: (-2:Unspecified error) File can't be opened for reading! in function 'read'\n"]}]},{"cell_type":"markdown","metadata":{"id":"omzRngO32zd1"},"source":["# **Display Detected Images**"]},{"cell_type":"code","metadata":{"id":"QDq9tZb9ysHX","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1730823027572,"user_tz":-330,"elapsed":616,"user":{"displayName":"Devi S Naidu","userId":"13934691048356277861"}},"outputId":"b1549437-243b-4366-8f6d-2c90797e2abe"},"source":["import cv2\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","image = cv2.imread(\"dha.jpg\")\n","height, width = image.shape[:2]\n","resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n","\n","fig = plt.gcf()\n","fig.set_size_inches(18, 10)\n","plt.axis(\"off\")\n","plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n","plt.show()"],"execution_count":25,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'NoneType' object has no attribute 'shape'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-e312da69e8ec>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dha.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mresized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"]}]}]}